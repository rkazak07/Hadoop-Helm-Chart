1. You can check the status of HDFS by running this command:
   kubectl exec -n {{ .Release.Namespace }} -it {{ include "hadoop.fullname" . }}-hdfs-nn-0 -- /opt/hadoop/bin/hdfs dfsadmin -report
   
   kubectl exec -n {{ .Release.Namespace }} -it {{ include "hadoop.fullname" . }}-hdfs-nn-0 -- /opt/hadoop/bin/hdfs getconf -namenodes

2. To inquire about the status of secondary namenodes
   kubectl exec -n {{ .Release.Namespace }} -it {{ include "hadoop.fullname" . }}-hdfs-nn-0 -- /opt/hadoop/bin/hdfs getconf -secondaryNamenodes
    
3. For the HDFS file listing and insertion command
   kubectl exec -n {{ .Release.Namespace }} -it {{ include "hadoop.fullname" . }}-hdfs-nn-0 -- /opt/hadoop/bin/hdfs dfs -ls /
   
   kubectl exec -n {{ .Release.Namespace }} -it {{ include "hadoop.fullname" . }}-hdfs-nn-0 -- /opt/hadoop/bin/hdfs dfs -mkdir  /foldername
    
   

3. You can list the yarn nodes by running this command:
   kubectl exec -n {{ .Release.Namespace }} -it {{ include "hadoop.fullname" . }}-yarn-rm-0 -- /opt/hadoop/bin/yarn node -list

4. Create a port-forward to the yarn resource manager UI:
   kubectl port-forward -n {{ .Release.Namespace }} {{ include "hadoop.fullname" . }}-yarn-rm-0 8088:8088

   Then open the ui in your browser:

   open http://localhost:8088

5. You can run included hadoop tests like this:
   kubectl exec -n {{ .Release.Namespace }} -it {{ include "hadoop.fullname" . }}-yarn-nm-0 -- /opt/hadoop/bin/hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-{{ .Values.hadoopVersion }}-tests.jar TestDFSIO -write -nrFiles 5 -fileSize 128MB -resFile /tmp/TestDFSIOwrite.txt

6. You can list the mapreduce jobs like this:
   kubectl exec -n {{ .Release.Namespace }} -it {{ include "hadoop.fullname" . }}-yarn-rm-0 -- /opt/hadoop/bin/mapred job -list


7. You can scale the number of yarn nodes like this:
   helm upgrade {{ .Release.Name }} --set yarn.nodeManager.replicas=4 .

   Make sure to update the values.yaml if you want to make this permanent.
